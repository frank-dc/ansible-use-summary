# socket 网络模型的 I/O 多路复用
- [socket 网络模型的 I/O 多路复用](#socket-网络模型的-io-多路复用)
  - [概括](#概括)
  - [来源](#来源)
## 概括
最基础的 TCP 的 socket 编程，它是阻塞 I/O 模型，基本上只能一对一通信，为了服务更多的客户端，我们需要改进网络 I/O 模型。

比较传统的方式是使用多进程/线程模型，每来一个客户端连接，就分配一个进程/线程，然后后续的读写都在对应的进程/线程完成，这种方式处理一定数量（比如100个）的客户端没问题，但是当客户端增加到一万，甚至更多，那么一万个进程/线程的`调度`、`上下文切换`以及`占用的内存`都会成为瓶颈。

为了解决上面的👆🏻问题，就出现了 I/O 多路复用，可以在一个进程/线程里处理多个文件的 I/O（类似一个 CPU 并发多个进程）, Linux 有三种提供 I/O 多路复用的 API，分别是`select`、`poll`、`epoll`。

* select/poll

select 和 poll 没有本质区别，都是使用`线性结构`来存储进程关注的 socket（文件描述符） 集合。

在使用的时候，首先需要把关注的 socket 集合通过 select/poll 系统调用从用户态拷贝到内核态，由内核态检测事件，当有网络事件产生时，内核需要遍历进程关注的 socket 集合，找到对应的 socket，并设置其状态可读/可写，然后把整个 socket 集合从内核态拷贝到用户态，用户态还要继续遍历 socket 集合找到可读/可写的 socket，然后对其处理。

select 和 poll 的缺陷在于，当客户端越多，也就是 socket 集合越大，对 socket 集合的遍历和拷贝会带来很大的开销，很难应对C10k[^1]。

[^1]: 优化网络套接字以同时处理大量客户端的问题。

* epoll

epoll 在内核里使用`红黑树`来关注进程所有待检测的 socket，红黑树是个高效的数据结构，增删查一般时间复杂度是O(logn)，通过对这颗红黑树的管理，不需要像 select/poll 在每次操作时都传入整个 socket 集合，减少了内核和用户空间大量数据的拷贝和内存分配。

epoll 使用事件驱动的机制，内核里维护了一个`链表`来记录就绪事件，只将有事件发生的 socket 集合传递给用户空间的应用程序，不需要像 select/poll 那样遍历整个 socket 集合（包含有和无事件的socket），大大提高了检测效率。

epoll 支持边缘触发[^2]和水平触发[^3]两种触发的方式，select/poll 只支持水平触发。

[^2]: 当被监控的 Socket 描述符上有可读事件发生时，服务器端只会从 epoll_wait 中苏醒一次，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完。

[^3]: 当被监控的 Socket 上有可读事件发生时，服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束，目的是告诉我们有数据需要读取。



## 来源
* [https://bbs.huaweicloud.com/blogs/279731](https://bbs.huaweicloud.com/blogs/279731)